{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground\n",
    "\n",
    "_We'll use this to prototype quickly on video generation._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from manim import *\n",
    "\n",
    "config.media_width = \"75%\"\n",
    "config.verbosity = \"WARNING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Manim Community <span style=\"color: #008000; text-decoration-color: #008000\">v0.18.1</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Manim Community \u001b[32mv0.\u001b[0m\u001b[32m18.1\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                 \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"media/jupyter/AttentionInLLMs@2024-10-05@10-46-55.mp4\" controls autoplay loop style=\"max-width: 75%;\"  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%manim -qm AttentionInLLMs\n",
    "\n",
    "class AttentionInLLMs(Scene):\n",
    "    def construct(self):\n",
    "        # Title\n",
    "        title = Text(\"Attention in Large Language Models\", font_size=40)\n",
    "        self.play(Write(title))\n",
    "        self.play(title.animate.to_edge(UP))\n",
    "        self.wait()\n",
    "\n",
    "        # Explain the basic concept\n",
    "        basic_concept = Text(\"Attention helps the model focus on relevant parts of the input\", \n",
    "                             font_size=24)\n",
    "        basic_concept.next_to(title, DOWN, buff=0.5)\n",
    "        self.play(Write(basic_concept))\n",
    "        self.wait(2)\n",
    "\n",
    "        # Create input sequence\n",
    "        input_text = Text(\"The cat sat on the mat\", font_size=24)\n",
    "        input_text.to_edge(LEFT).shift(UP)\n",
    "        self.play(Transform(basic_concept, input_text))\n",
    "        self.wait()\n",
    "\n",
    "        # Highlight words\n",
    "        highlights = [\n",
    "            SurroundingRectangle(input_text[4:7], color=YELLOW, buff=0.1),  # \"cat\"\n",
    "            SurroundingRectangle(input_text[-3:], color=GREEN, buff=0.1)    # \"mat\"\n",
    "        ]\n",
    "        self.play(*[Create(highlight) for highlight in highlights])\n",
    "        self.wait()\n",
    "\n",
    "        # Show attention weights\n",
    "        weights = VGroup(*[\n",
    "            Arrow(input_text.get_bottom(), input_text[-3:].get_top(), \n",
    "                  color=interpolate_color(RED, GREEN, i/5))\n",
    "            for i in range(6)\n",
    "        ])\n",
    "        self.play(LaggedStart(*[GrowArrow(weight) for weight in weights], lag_ratio=0.2))\n",
    "        self.wait()\n",
    "\n",
    "        # Clear previous elements\n",
    "        self.play(*[FadeOut(mob) for mob in self.mobjects if mob != title])\n",
    "\n",
    "        # Explain self-attention\n",
    "        self_attention = Text(\"Self-Attention: Each word attends to all words\", font_size=24)\n",
    "        self_attention.next_to(title, DOWN, buff=0.5)\n",
    "        self.play(Write(self_attention))\n",
    "        self.wait()\n",
    "\n",
    "        # Create a simple matrix\n",
    "        matrix = Matrix([\n",
    "            [\"1\", \"0.2\", \"0.1\"],\n",
    "            [\"0.2\", \"1\", \"0.3\"],\n",
    "            [\"0.1\", \"0.3\", \"1\"]\n",
    "        ])\n",
    "        matrix.scale(0.8)\n",
    "        matrix.next_to(self_attention, DOWN, buff=0.5)\n",
    "        self.play(Create(matrix))\n",
    "        self.wait()\n",
    "\n",
    "        # Label matrix\n",
    "        matrix_label = Text(\"Attention Matrix\", font_size=20)\n",
    "        matrix_label.next_to(matrix, DOWN)\n",
    "        self.play(Write(matrix_label))\n",
    "        self.wait()\n",
    "\n",
    "        # Show multi-head attention\n",
    "        multi_head = Text(\"Multi-Head Attention: Multiple attention mechanisms in parallel\", \n",
    "                          font_size=24)\n",
    "        multi_head.next_to(matrix_label, DOWN, buff=0.5)\n",
    "        self.play(Write(multi_head))\n",
    "        self.wait()\n",
    "\n",
    "        # Create multiple attention heads\n",
    "        heads = VGroup(*[\n",
    "            Circle(radius=0.2, color=BLUE, fill_opacity=0.5)\n",
    "            for _ in range(4)\n",
    "        ]).arrange(RIGHT, buff=0.3)\n",
    "        heads.next_to(multi_head, DOWN, buff=0.5)\n",
    "        self.play(Create(heads))\n",
    "        self.wait()\n",
    "\n",
    "        # Conclusion\n",
    "        conclusion = Text(\"Attention mechanisms enable LLMs to process and understand context\", \n",
    "                          font_size=24)\n",
    "        conclusion.next_to(heads, DOWN, buff=0.5)\n",
    "        self.play(Write(conclusion))\n",
    "        self.wait(2)\n",
    "\n",
    "        # Fade out everything\n",
    "        self.play(*[FadeOut(mob) for mob in self.mobjects])\n",
    "        self.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Manim Community <span style=\"color: #008000; text-decoration-color: #008000\">v0.18.1</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Manim Community \u001b[32mv0.\u001b[0m\u001b[32m18.1\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                    \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"media/jupyter/AttentionInLLMs@2024-10-05@10-49-53.mp4\" controls autoplay loop style=\"max-width: 75%;\"  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%manim -qm AttentionInLLMs\n",
    "\n",
    "class AttentionInLLMs(Scene):\n",
    "    def construct(self):\n",
    "        # Title\n",
    "        title = Text(\"Attention Mechanism in Large Language Models\", font_size=40)\n",
    "        self.play(Write(title))\n",
    "        self.play(title.animate.to_edge(UP))\n",
    "        self.wait()\n",
    "\n",
    "        # Explain attention mechanism\n",
    "        attention_text = Text(\n",
    "            \"Attention allows the model to focus on relevant parts of the input\",\n",
    "            font_size=24\n",
    "        )\n",
    "        attention_text.next_to(title, DOWN, buff=0.5)\n",
    "        self.play(Write(attention_text))\n",
    "        self.wait(2)\n",
    "\n",
    "        # Input sentence\n",
    "        sentence = Text(\"The cat sat on the mat\", font_size=24)\n",
    "        sentence.to_edge(LEFT).shift(UP)\n",
    "        self.play(Transform(attention_text, sentence))\n",
    "        self.wait()\n",
    "\n",
    "        # Highlight key words\n",
    "        cat_highlight = SurroundingRectangle(sentence[1], color=YELLOW, buff=0.1)\n",
    "        mat_highlight = SurroundingRectangle(sentence[5], color=GREEN, buff=0.1)\n",
    "        self.play(Create(cat_highlight), Create(mat_highlight))\n",
    "        self.wait()\n",
    "\n",
    "        # Show attention arrows\n",
    "        arrows = VGroup(*[\n",
    "            Arrow(\n",
    "                cat_highlight.get_bottom(),\n",
    "                mat_highlight.get_top(),\n",
    "                color=interpolate_color(RED, GREEN, i / 5)\n",
    "            )\n",
    "            for i in range(5)\n",
    "        ])\n",
    "        self.play(LaggedStart(*[GrowArrow(arrow) for arrow in arrows], lag_ratio=0.1))\n",
    "        self.wait()\n",
    "\n",
    "        # Self-Attention explanation\n",
    "        self_attention = Text(\"Self-Attention: Each word attends to all words\", font_size=24)\n",
    "        self_attention.next_to(title, DOWN, buff=0.5)\n",
    "        self.play(Transform(attention_text, self_attention))\n",
    "        self.wait()\n",
    "\n",
    "        # Attention matrix visualization\n",
    "        matrix = Matrix([\n",
    "            [\"1\", \"0.2\", \"0.1\", \"0.3\", \"0.4\"],\n",
    "            [\"0.2\", \"1\", \"0.3\", \"0.5\", \"0.2\"],\n",
    "            [\"0.1\", \"0.3\", \"1\", \"0.2\", \"0.1\"],\n",
    "            [\"0.3\", \"0.5\", \"0.2\", \"1\", \"0.3\"],\n",
    "            [\"0.4\", \"0.2\", \"0.1\", \"0.3\", \"1\"]\n",
    "        ])\n",
    "        matrix.scale(0.8)\n",
    "        matrix.next_to(self_attention, DOWN, buff=0.5)\n",
    "        self.play(Create(matrix))\n",
    "        self.wait()\n",
    "\n",
    "        # Highlight matrix\n",
    "        matrix_highlight = SurroundingRectangle(matrix, color=BLUE, buff=0.1)\n",
    "        self.play(Create(matrix_highlight))\n",
    "        self.wait()\n",
    "\n",
    "        # Conclusion\n",
    "        conclusion = Text(\n",
    "            \"Attention mechanisms enable LLMs to understand context effectively\",\n",
    "            font_size=24\n",
    "        )\n",
    "        conclusion.next_to(matrix, DOWN, buff=0.5)\n",
    "        self.play(Write(conclusion))\n",
    "        self.wait(2)\n",
    "\n",
    "        # Fade out\n",
    "        self.play(*[FadeOut(mob) for mob in self.mobjects])\n",
    "        self.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crewAIv0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
